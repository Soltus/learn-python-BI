{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "#构建中文分词类v3.0\n",
    "class Leijieba:    \n",
    "    \n",
    "    __stopwords = ''\n",
    "        \n",
    "    def __init__(self,s,t='',b=''):    #t为停用词的文件参数（路径+文件名）；b为本地词库的文件参数（路径+文件名）\n",
    "        self.__s = s\n",
    "        \n",
    "        if t != '':\n",
    "            self.__stopwords = [line.strip() for line in open(t,encoding='UTF-8').readlines()]  #逐行读取停用词文件，放入类内部变量__stopwords\n",
    "            \n",
    "        if b != '':\n",
    "            jieba.load_userdict(b)\n",
    "        \n",
    "    def cut(self):\n",
    "        seg_list = jieba.cut(self.__s, cut_all=False)\n",
    "        if self.__stopwords != '':\n",
    "            outstr = []    #目的是存储去掉停用用词的分词结果\n",
    "            for word in list(seg_list):\n",
    "                if word not in self.__stopwords:\n",
    "                    if word.strip() !='':   #去空格\n",
    "                        outstr.append(word)\n",
    "                        \n",
    "            seg_list = outstr                                                \n",
    "        return list(seg_list)\n",
    "    \n",
    "    def num(self):\n",
    "        lst = self.cut()\n",
    "        \n",
    "        word_dict = {}        \n",
    "        for item in lst:\n",
    "            if item not in word_dict:\n",
    "                word_dict[item] = 1   #新建“键-值”\n",
    "            else:\n",
    "                word_dict[item] += 1  #增加 键的值\n",
    "        return sorted(word_dict.items(), key=lambda e:e[1], reverse=True)  \n",
    "    \n",
    "    def cut1(self,num = 0,sw = 0):   #1.删除词频低于lnum的词,2.if sw ==1,删除单字的词\n",
    "        lst = self.cut()\n",
    "        \n",
    "        word_dict = {}        \n",
    "        for item in lst:\n",
    "            if item not in word_dict:\n",
    "                word_dict[item] = 1   #新建“键-值”\n",
    "            else:\n",
    "                word_dict[item] += 1  #增加 键的值\n",
    "        dic = dict(sorted(word_dict.items(), key=lambda e:e[1], reverse=True)) \n",
    "        lst1 = lst[:]\n",
    "        \n",
    "        if sw == 1:\n",
    "            for item in lst:\n",
    "                if len(item) == 1:\n",
    "                    lst1.remove(item)\n",
    "                else:\n",
    "                    if dic[item] <= num:\n",
    "                        lst1.remove(item)\n",
    "        else:\n",
    "            for item in lst:\n",
    "                if dic[item] <= num:\n",
    "                    lst1.remove(item)\n",
    "                \n",
    "        return lst1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "txtfile = pd.read_csv('cnews.txt', sep='\\t', encoding = 'utf-8', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(txtfile)):\n",
    "    txtfile[1][i] = re.sub(r'[^\\u4e00-\\u9fa5]+',' ',txtfile[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lb\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.620 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "s = 'AAA我是分词的间隔符BBB'.join(list(txtfile[1]))\n",
    "m = Leijieba(s,t='stopwords.txt',b='userdic.txt')\n",
    "s1 = ' '.join(m.cut1(10,1))\n",
    "txtf = open('cnews.jieba.txt','wt', encoding='utf-8')\n",
    "txtf.write(s1.replace('AAA我是分词的间隔符BBB','\\n'))\n",
    "txtf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = pd.read_csv('cnews.jieba.txt', sep='\\t', encoding = 'utf-8', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "vectorizer.fit(list(txt[0]))\n",
    "X = vectorizer.transform(list(txt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ordering = ['体育','娱乐','家居','房产','教育','时尚','时政','游戏','科技','财经']\n",
    "y = np.array(txtfile[0].map(lambda x: ordering.index(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          体育       1.00      0.98      0.99       124\n",
      "          娱乐       0.94      0.98      0.96       121\n",
      "          家居       0.97      0.90      0.93       127\n",
      "          房产       0.99      0.93      0.96       150\n",
      "          教育       0.97      1.00      0.99       108\n",
      "          时尚       0.93      0.98      0.95       129\n",
      "          时政       0.95      0.92      0.94       111\n",
      "          游戏       1.00      0.99      1.00       130\n",
      "          科技       0.99      1.00      1.00       128\n",
      "          财经       0.95      1.00      0.97       122\n",
      "\n",
      "    accuracy                           0.97      1250\n",
      "   macro avg       0.97      0.97      0.97      1250\n",
      "weighted avg       0.97      0.97      0.97      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "t = classification_report(y_test, y_pred, target_names = ordering)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
